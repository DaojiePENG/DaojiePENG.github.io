<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="LOVON: Legged Open-Vocabulary Object Navigator.">
  <meta name="keywords" content="Robotics, Open-Vocabulary, Vision-Language Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LOVON: Legged Open-Vocabulary Object Navigator</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/HKUST-Logo-icon_0.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/DaojiePENG">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Information
        </a>
        <div class="navbar-dropdown">
          
          <a class="navbar-item" href="https://www.hkust-gz.edu.cn/zh/?variant=zh-cn/">
            HKUST-GZ
          </a>
          <a class="navbar-item" href="https://personal.hkust-gz.edu.cn/junma/people-page.html">
            People in Our Lab
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LOVON: Legged Open-Vocabulary Object Navigator</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://daojiepeng.github.io/Personal/">Daojie Peng</a><sup>1*</sup>,
            </span>
            <span class="author-block">
              <a href="https://andycao1125.github.io/">Jiahang Cao</a><sup>1,2*</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/jonyzhang2023">Qiang Zhang</a><sup>1,2*</sup>,
            </span>
            <span class="author-block">
              <a href="https://personal.hkust-gz.edu.cn/junma/index.html">Jun MA</a><sup>1,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The Hong Kong University of Science and Technology (Guangzhou)</span>
            <br>
            <span class="author-block"><sup>2</sup>Beijing Innovation Center of Humanoid Robotics</span>
            <br>
            <span class="author-block"><sup>3</sup>The Hong Kong University of Science and Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="./static/paper/LOVON_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://youtu.be/j1IG6lxIaWY"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/DaojiePENG/LOVON"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" src="./static/images/teaser.png" alt="LOVON系统介绍" width="100%">
      <!-- <embed id="teaser" src="./static/images/teaser.pdf" type="application/pdf" width="100%" height="600px" /> -->

      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->

      <h2 class="subtitle has-text-centered">
        <span class="dnerf">LOVON</span> is a novel system that integrates LLMs for hierarchical task planning 
        with open-vocabulary visual detection and legged robot mobility.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="run to the backpack at speed of 0.35 m/s" id="backpack" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/01_backpack.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="move to the person at speed of 0.5 m/s" id="person" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/18_b2_person.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/15_h1_person.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/03_human_office.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/04_frige.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/07_swing_stair.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/08_bench.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/09_plants.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/10_bike.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/11_car.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/12_ball.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/16_h1_chair.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/20_chair_kick.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/25_dog.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
<!--           <p>
            We propose <span class="dnerf">LOVON</span>, a novel system that integrates LLMs for hierarchical task planning 
            with open-vocabulary visual detection and legged robot mobility. 
          </p> -->
          <p>
            Object navigation in open-world environments remain a critical challenge for robotic systems. Despite advancements 
            in large language models (LLMs) for task planning, open-vocabulary vision models for object detection, and versatile 
            legged robots capable of traversing complex terrains, existing approaches lack a unified navigation framework to execute 
            composite long-range missions. We propose <i>LOVON</i>, a novel system that integrates LLMs for hierarchical task planning 
            with open-vocabulary visual detection and legged robot mobility. To address real-world challenges including visual jittering, 
            blind zones, and temporary target loss, we design dedicated solutions such as Laplacian Variance Filtering for visual stabilization. 
            Extensive evaluations on Go2, B2, and H1-2 legged platforms demonstrate successful completion of long-sequence tasks involving 
            real-time detection, search, and navigation toward open-vocabulary dynamic targets. To the best of our knowledge, this work presents 
            the first operational system achieving such capabilities in unstructured environments. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
           <iframe width="560" height="315" src="https://www.youtube.com/embed/j1IG6lxIaWY?si=DmvMBZ4ji_p_tFo5" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <!-- Paper Pipeline figures. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">LOVON Pipeline</h2>
        <div class="content has-text-justified">
        <p>
          The LOVON system is designed to integrate LLMs for hierarchical task planning with open-vocabulary visual detection and legged robot mobility.
          The pipeline consists of several key components, including visual stabilization, object detection, and navigation planning.
          The system is capable of operating in various environments, including indoor and outdoor settings, and can handle dynamic objects in real-time.
        </p>
        </div>
        <img src="./static/images/pipeline.png" alt="LOVON Pipeline" width="100%">
      </div>
  </div>

    <!--/ Paper Baseline table. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Simulation Results</h2>
        <div class="content has-text-justified">
        <p>
          As shown in the Table, our method, LOVON, outperforms several baseline approaches,
          achieving a perfect SR of 1.00 across most environments, including ParkingLot, UrbanCity and SnowVillage. 
          Compared to EVT, LOVON demonstrates superior tracking performance, e.g., 500/1.00 vs. 484/0.92 in ParkingLot. 
          Even when compared to the state-of-the-art TrackVLA, which achieves 1.00 SR but requires 360 hours of training, 
          LOVON stands out with an efficient training time of just 1.5 hours, offering both high accuracy and significant efficiency.
        </p>
      </div>
        <img src="./static/images/baseline.png" alt="LOVON Baseline" width="100%">
      </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Multi-Embodiment</h2>
          <p>
            LOVON is designed to be a multi-embodiment system, capable of operating on various legged robots.
            Here we show examples of LOVON running on the H1-2 robot, which is a humanoid robot with a
            bipedal structure, the Go2 and B2, which are quadruped robots. 
            The robots are able to navigate through a complex environment, detecting and tracking objects in real-time.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/multi-embodiment.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Open-World Object Seeking</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              LOVON is capable of operating in various environments, including indoor and outdoor settings.
              Here we show examples of LOVON running in multi-environments including indoors like office, lib, 
              tea room, stairs, etc.; outdoors like parking area, playground, wild grass, etc.;
              The robot is able to transverse through the sand and grass, detecting and tracking backpack on 
              the playground in real-time.
            </p>
            <video id="matting-video" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/open-world seeking.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Robustness</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Recapture the Lost Target</h3>
        <div class="content has-text-justified">
          <p>
            LOVON is robust to visual disturbances such as motion blur, occlusion, and dynamic state changes.
            Here we show an example of blocking out the umbrella in the scene, which causes the
            visual effects to be disturbed. LOVON is able to recover from this disturbance
            and continue approaching the target.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/umbrella_recapture.mp4"
                    type="video/mp4">
          </video>
        </div>


        <!-- Re-rendering. -->
        <h3 class="title is-4">Dynamic Tracking</h3>
        <div class="content has-text-justified">
          <p>
            LOVON can also be used to track dynamic objects in complex unstructured environments.
            Here we show an example of tracking a person in real-world wild grass. The person is
            moving around in the scene, and LOVON is able to track the person and render the scene
            with the person in it. 
            We also show an example of tracking a person with h1-2 robot in a real-world environment.
            The robot is able to detect and track the person in real-time, even when the person is moving around. And stay a safe distance from the person.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/dynamic_tracking_person.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/15_h1_person.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->

    <h3 class="title is-4">Challenging Terrain</h3>
        <div class="content has-text-justified">
          <p>
            LOVON is capable of navigating through challenging terrains such as stairs,
            uneven surfaces, and gravel ground. Here we show an example of LOVON navigating through a spiral staircase.
            The robot is able to detect and track the target while navigating through the stairs.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/07_swing_stair.mp4"
                    type="video/mp4">
          </video>
        </div>


    <!-- Concurrent Work. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{daojie2025lovon,
  title={LOVON: Legged Open-Vocabulary Object Navigator},
  author={Peng, Daojie and Cao, Jiahang and Zhang, Qiang and Ma, Jun},
  journal={arxiv},
  year={2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/paper/LOVON_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/DaojiePENG" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p> -->
          <p>
            Referring to the <a href="https://github.com/nerfies/nerfies.github.io">source code</a> for the website template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
